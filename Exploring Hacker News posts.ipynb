{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Hacker News* posts strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll work with a data set of submissions to popular technology site [Hacker News](https://news.ycombinator.com).\n",
    "\n",
    "Hacker News is a site started by the startup incubator [Y Combinator](https://www.ycombinator.com), where user-submitted stories (known as \"posts\") are voted and commented upon, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "__Through our data analysis of HN, we'll identify the best post strategy to get the most comments and so attract the most visibility.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set : [Link](https://www.kaggle.com/hacker-news/hacker-news-posts). It should be noted that it has been reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that did not receive any comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two types of posts we'll explore begin with either Ask HN or Show HN.\n",
    "Users submit Ask HN posts to ask the Hacker News community a specific question, such as \"What is the best online course you've ever taken?\" Likewise, users submit Show HN posts to show the Hacker News community a project, product, or just generally something interesting.\n",
    "\n",
    "We'll specifically compare these two types of posts to determine the following:\n",
    "* Between Ask HN and Show HN, which one receive more comments on average\n",
    "* Do posts created at a certain time receive more comments on average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'],\n",
       " ['12224879',\n",
       "  'Interactive Dynamic Video',\n",
       "  'http://www.interactivedynamicvideo.com/',\n",
       "  '386',\n",
       "  '52',\n",
       "  'ne0phyte',\n",
       "  '8/4/2016 11:52'],\n",
       " ['10975351',\n",
       "  'How to Use Open Source and Shut the Fuck Up at the Same Time',\n",
       "  'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/',\n",
       "  '39',\n",
       "  '10',\n",
       "  'josep2',\n",
       "  '1/26/2016 19:30'],\n",
       " ['11964716',\n",
       "  \"Florida DJs May Face Felony for April Fools' Water Joke\",\n",
       "  'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/',\n",
       "  '2',\n",
       "  '1',\n",
       "  'vezycash',\n",
       "  '6/23/2016 22:20'],\n",
       " ['11919867',\n",
       "  'Technology ventures: From Idea to Enterprise',\n",
       "  'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
       "  '3',\n",
       "  '1',\n",
       "  'hswarna',\n",
       "  '6/17/2016 0:01']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "open_file = open('hacker_news.csv')\n",
    "hn = csv.reader(open_file)\n",
    "hn = list(hn)\n",
    "hn[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing Headers from the List of Lists__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12224879',\n",
       "  'Interactive Dynamic Video',\n",
       "  'http://www.interactivedynamicvideo.com/',\n",
       "  '386',\n",
       "  '52',\n",
       "  'ne0phyte',\n",
       "  '8/4/2016 11:52'],\n",
       " ['10975351',\n",
       "  'How to Use Open Source and Shut the Fuck Up at the Same Time',\n",
       "  'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/',\n",
       "  '39',\n",
       "  '10',\n",
       "  'josep2',\n",
       "  '1/26/2016 19:30'],\n",
       " ['11964716',\n",
       "  \"Florida DJs May Face Felony for April Fools' Water Joke\",\n",
       "  'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/',\n",
       "  '2',\n",
       "  '1',\n",
       "  'vezycash',\n",
       "  '6/23/2016 22:20'],\n",
       " ['11919867',\n",
       "  'Technology ventures: From Idea to Enterprise',\n",
       "  'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',\n",
       "  '3',\n",
       "  '1',\n",
       "  'hswarna',\n",
       "  '6/17/2016 0:01'],\n",
       " ['10301696',\n",
       "  'Note by Note: The Making of Steinway L1037 (2007)',\n",
       "  'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',\n",
       "  '8',\n",
       "  '2',\n",
       "  'walterbell',\n",
       "  '9/30/2015 4:12']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn = hn[1:]\n",
    "hn[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Ask HN and Show HN Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    title = title.lower()\n",
    "    if title.startswith(\"ask hn\"):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith(\"show hn\"):\n",
    "        show_posts.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10627194',\n",
       "  'Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform',\n",
       "  'https://iot.seeed.cc',\n",
       "  '26',\n",
       "  '22',\n",
       "  'kfihihc',\n",
       "  '11/25/2015 14:03'],\n",
       " ['10646440',\n",
       "  'Show HN: Something pointless I made',\n",
       "  'http://dn.ht/picklecat/',\n",
       "  '747',\n",
       "  '102',\n",
       "  'dhotson',\n",
       "  '11/29/2015 22:46']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_posts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1744"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ask_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(show_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Average Number of Comments for Ask HN and Show HN Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.038417431192661\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    num_comments = row[4]\n",
    "    num_comments = int(num_comments)\n",
    "    total_ask_comments += num_comments\n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "print(avg_ask_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_show_comments = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    num_comments = row[4]\n",
    "    num_comments = int(num_comments)\n",
    "    total_show_comments += num_comments\n",
    "\n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "print(avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just determined that, on average, ask posts receive more comments than show posts. Since ask posts are more likely to receive comments, we'll focus our remaining analysis just on these posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What time is the optimum time to post for attracting comments ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll determine if ask posts created at a certain time are more likely to attract comments. We'll use the following steps to perform this analysis:\n",
    "\n",
    "1. Calculate the amount of ask posts created in each hour of the day, along with the number of comments received.\n",
    "2. Calculate the average number of comments ask posts receive by hour created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding the Amount of Ask Posts and Comments by Hour Created__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculating the amount of ask posts and comments by hour created\n",
    "from datetime import datetime\n",
    "\n",
    "result_list = [] \n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    comments = int(row[4])\n",
    "    result_list.append([created_at, comments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    comments = row[1]\n",
    "    time = datetime.strptime(date, \"%m/%d/%Y %H:%M\").strftime(\"%H\")\n",
    "    \n",
    "    if time in counts_by_hour:\n",
    "        counts_by_hour[time] += 1\n",
    "        comments_by_hour[time] += comments\n",
    "    else:\n",
    "        counts_by_hour[time] = 1\n",
    "        comments_by_hour[time] = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'21': 1745, '12': 687, '18': 1439, '05': 464, '04': 337, '22': 479, '03': 421, '09': 251, '01': 683, '16': 1814, '02': 1381, '06': 397, '15': 4477, '08': 492, '13': 1253, '00': 447, '17': 1146, '20': 1722, '14': 1416, '07': 267, '19': 1188, '23': 543, '10': 793, '11': 641}\n"
     ]
    }
   ],
   "source": [
    "print(comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Average number of comments for ask posts created during each hour of the day__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.009, '21'], [9.411, '12'], [13.202, '18'], [10.087, '05'], [7.17, '04'], [6.746, '22'], [7.796, '03'], [5.578, '09'], [11.383, '01'], [16.796, '16'], [23.81, '02'], [9.023, '06'], [38.595, '15'], [10.25, '08'], [14.741, '13'], [8.127, '00'], [11.46, '17'], [21.525, '20'], [13.234, '14'], [7.853, '07'], [10.8, '19'], [7.985, '23'], [13.441, '10'], [11.052, '11']]\n"
     ]
    }
   ],
   "source": [
    "average_comments_by_hour = [[round(comments_by_hour[x]/counts_by_hour[x],3), x] for x in counts_by_hour]\n",
    "\n",
    "print(average_comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sorting and Printing Values from a List of Lists__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format makes it hard to identify the hours with the highest values. Let's finish by sorting the list of lists and printing the five highest values in a format that's easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:00: 38.59 average comments per post\n",
      "02:00: 23.81 average comments per post\n",
      "20:00: 21.52 average comments per post\n",
      "16:00: 16.80 average comments per post\n",
      "21:00: 16.01 average comments per post\n",
      "13:00: 14.74 average comments per post\n",
      "10:00: 13.44 average comments per post\n",
      "14:00: 13.23 average comments per post\n",
      "18:00: 13.20 average comments per post\n",
      "17:00: 11.46 average comments per post\n",
      "01:00: 11.38 average comments per post\n",
      "11:00: 11.05 average comments per post\n",
      "19:00: 10.80 average comments per post\n",
      "08:00: 10.25 average comments per post\n",
      "05:00: 10.09 average comments per post\n",
      "12:00: 9.41 average comments per post\n",
      "06:00: 9.02 average comments per post\n",
      "00:00: 8.13 average comments per post\n",
      "23:00: 7.99 average comments per post\n",
      "07:00: 7.85 average comments per post\n",
      "03:00: 7.80 average comments per post\n",
      "04:00: 7.17 average comments per post\n",
      "22:00: 6.75 average comments per post\n",
      "09:00: 5.58 average comments per post\n"
     ]
    }
   ],
   "source": [
    "swap_average_by_hour = [[h[0],h[1]] for h in average_comments_by_hour]\n",
    "sorted_swap = sorted(average_comments_by_hour, reverse=True)\n",
    "for avg, h in sorted_swap:\n",
    "    time = datetime.strptime(h, \"%H\").strftime(\"%H:%M\")\n",
    "    print(\"{}: {:.2f} average comments per post\".format(time, avg)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask HN posts receive the most comments if posted in the 3pm hour. To optimize the chance to attract the most comments, the best type of posts is ask HN post which leads to more reactions and the optimum time to post is 3pm. This result is appealing given that there is about a 62% increase in the number of comments between the hours with the highest and second highest average number of comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "For this part, we work with the same dataset - submissions users made to Hacker News from 2006 to 2015. But we use a dataset which has been sampled to 3000 rows from the data randomly, and and in which all of the extraneous columns have been removed.\n",
    "\n",
    "Our data only has four columns:\n",
    "\n",
    "- submission_time - When the article was submitted\n",
    "- upvotes - The number of upvotes the article received\n",
    "- url - The base URL of the article\n",
    "- headline - The article's headline\n",
    "\n",
    "#### Our goal is to discover which types of articles tend to be the most popular on Hacker News. \n",
    "To do so, we'll analyse the correlation between headlines and upvotes. Since upvotes are an indicator of popularity, we'll be predicting the number of upvotes the articles received, based on their headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2014-06-24T05:50:40.000Z',\n",
       "  '1',\n",
       "  'flux7.com',\n",
       "  '8 Ways to Use Docker in the Real World'],\n",
       " ['2010-02-17T16:57:59Z',\n",
       "  '1',\n",
       "  'blog.jonasbandi.net',\n",
       "  'Software: Sadly we did adopt from the construction analogy'],\n",
       " ['2014-02-04T02:36:30Z',\n",
       "  '1',\n",
       "  'blogs.wsj.com',\n",
       "  ' Google’s Stock Split Means More Control for Larry and Sergey '],\n",
       " ['2011-10-26T07:11:29Z',\n",
       "  '1',\n",
       "  'threatpost.com',\n",
       "  'SSL DOS attack tool released exploiting negotiation overhead'],\n",
       " ['2011-04-03T15:43:44Z',\n",
       "  '67',\n",
       "  'algorithm.com.au',\n",
       "  'Immutability and Blocks Lambdas and Closures']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "open_file = open('sel_hn_stories.csv')\n",
    "stories = csv.reader(open_file)\n",
    "stories = list(stories)\n",
    "\n",
    "stories[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        submission_time  upvotes                  url  \\\n",
      "0  2010-02-17T16:57:59Z        1  blog.jonasbandi.net   \n",
      "1  2014-02-04T02:36:30Z        1        blogs.wsj.com   \n",
      "2  2011-10-26T07:11:29Z        1       threatpost.com   \n",
      "3  2011-04-03T15:43:44Z       67     algorithm.com.au   \n",
      "4  2013-01-13T16:49:20Z        1      winmacsofts.com   \n",
      "\n",
      "                                            headline  \n",
      "0  Software: Sadly we did adopt from the construc...  \n",
      "1   Google’s Stock Split Means More Control for L...  \n",
      "2  SSL DOS attack tool released exploiting negoti...  \n",
      "3       Immutability and Blocks Lambdas and Closures  \n",
      "4         Comment optimiser la vitesse de Wordpress?  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "submissions = pd.read_csv(\"sel_hn_stories.csv\")\n",
    "submissions.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]\n",
    "submissions = submissions.dropna()\n",
    "print(submissions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Headlines\n",
    "\n",
    "We'll train a linear regression algorithm that predicts the number of upvotes a headline would receive. To do this, we'll need to convert each headline to a numerical representation.\n",
    "\n",
    "In tokenization, we break a sentence up into disconnected words. With tokenization, all we're doing is splitting each sentence into a list of individual words, or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Software:', 'Sadly', 'we', 'did', 'adopt', 'from', 'the', 'construction', 'analogy'], ['Google’s', 'Stock', 'Split', 'Means', 'More', 'Control', 'for', 'Larry', 'and', 'Sergey'], ['SSL', 'DOS', 'attack', 'tool', 'released', 'exploiting', 'negotiation', 'overhead'], ['Immutability', 'and', 'Blocks', 'Lambdas', 'and', 'Closures'], ['Comment', 'optimiser', 'la', 'vitesse', 'de', 'Wordpress?']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_headlines = []\n",
    "for item in submissions[\"headline\"]:\n",
    "    tokenized_headlines.append(item.split())\n",
    "    \n",
    "print(tokenized_headlines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tokens to Increase Accuracy\n",
    "\n",
    "We now have tokens, but we need to process them a bit to make our predictions more accurate. \n",
    "\n",
    "We'll need to convert those variations so that they're consistent.\n",
    "We can do this by lowercasing and also by removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['software', 'sadly', 'we', 'did', 'adopt', 'from', 'the', 'construction', 'analogy'], ['googles', 'stock', 'split', 'means', 'more', 'control', 'for', 'larry', 'and', 'sergey'], ['ssl', 'dos', 'attack', 'tool', 'released', 'exploiting', 'negotiation', 'overhead'], ['immutability', 'and', 'blocks', 'lambdas', 'and', 'closures'], ['comment', 'optimiser', 'la', 'vitesse', 'de', 'wordpress']]\n"
     ]
    }
   ],
   "source": [
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []\n",
    "for item in tokenized_headlines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc, \"\")\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)\n",
    "    \n",
    "print(clean_tokenized[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling a Matrix of Unique Words\n",
    "\n",
    "Now that we have our tokens, we can begin converting the sentences to their numerical representations. \n",
    "\n",
    "First, we'll retrieve all of the unique words from all of the headlines (tokens that only occur once don't add anything to the model's prediction power, and removing them will make our algorithm run much more quickly).\n",
    "\n",
    "Then, we'll create a matrix, and assign those words as the column headers. We'll initialize all of the values in the matrix to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  for  as  you  is  the  split  good  how  what    ...     frameworks  \\\n",
      "0    0    0   0    0   0    0      0     0    0     0    ...              0   \n",
      "1    0    0   0    0   0    0      0     0    0     0    ...              0   \n",
      "2    0    0   0    0   0    0      0     0    0     0    ...              0   \n",
      "3    0    0   0    0   0    0      0     0    0     0    ...              0   \n",
      "4    0    0   0    0   0    0      0     0    0     0    ...              0   \n",
      "\n",
      "   animated  walks  auctions  clouds  hammer  autonomous  vehicle  \\\n",
      "0         0      0         0       0       0           0        0   \n",
      "1         0      0         0       0       0           0        0   \n",
      "2         0      0         0       0       0           0        0   \n",
      "3         0      0         0       0       0           0        0   \n",
      "4         0      0         0       0       0           0        0   \n",
      "\n",
      "   crowdsourcing  disaster  \n",
      "0              0         0  \n",
      "1              0         0  \n",
      "2              0         0  \n",
      "3              0         0  \n",
      "4              0         0  \n",
      "\n",
      "[5 rows x 2310 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for tokens in clean_tokenized:\n",
    "    for token in tokens:\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        elif token in single_tokens and token not in unique_tokens:\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)\n",
    "print(counts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Token Occurences\n",
    "\n",
    "Now that we have a matrix where all values are 0, we need to fill in the correct counts for each cell. This involves going through each set of tokens, and incrementing the column counters in the appropriate row.\n",
    "\n",
    "When we're finished, we'll have a row vector for each headline that tells us how many times each token occured in that headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(clean_tokenized):\n",
    "    for token in item:\n",
    "        if token in unique_tokens:\n",
    "            counts.iloc[i][token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Columns to Increase Accuracy\n",
    "\n",
    "We have over many columns in our matrix. This can make it very hard for a linear regression model to make good predictions. Too many columns will cause the model to fit to noise instead of the signal in the data.\n",
    "\n",
    "There are two kinds of features that will reduce prediction accuracy.\n",
    "\n",
    "- Features that occur only a few times will cause overfitting, because the model doesn't have enough information to accurately decide whether they're important.\n",
    "\n",
    "- Features that occur too many times can also cause issues. These are words like and and to, which occur in nearly every headline. These words don't add any information, because they don't necessarily correlate with upvotes.\n",
    "\n",
    "To reduce the number of features and enable the linear regression model to make better predictions, we'll remove any words that occur fewer than 5 times or more than 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = counts.sum(axis=0)\n",
    "\n",
    "counts = counts.loc[:,(word_counts >= 5) & (word_counts <= 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data Into Train and Test Sets\n",
    "\n",
    "Now we'll need to split the data into two sets so that we can evaluate our algorithm effectively. We'll train our algorithm on a training set, then test its performance on a test set.\n",
    "\n",
    "The train_test_split() function from scikit-learn will help us accomplish this.\n",
    "\n",
    "We'll pass in .2 for the test_size parameter to randomly select 20% of the rows for our test set, and 80% for our training set.\n",
    "\n",
    "X_train and X_test contain the predictors, and y_train and y_test contain the value we're trying to predict (upvotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions With fit()\n",
    "\n",
    "Now that we have a training set and a test set, let's train a model and make test predictions. We'll use a linear regression algorithm from scikit-learn.\n",
    "\n",
    "First we'll initialize the model using the LinearRegression class. Then, we'll use the fit() method on the model to train with X_train and y_train. Finally, we'll make predictions with X_test.\n",
    "\n",
    "When we make predictions with a linear regression model, the model assigns coefficients to each column. Essentially, the model is determining which words correlate with more upvotes, and which with less. By finding these correlations, the model will be able to predict which headlines will be highly upvoted in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.17690686e+01  6.35049729e+01 -1.67007237e+01  1.67866575e+01\n",
      " -1.97586441e+00  3.44558067e+01 -4.49860607e+01  1.41788903e+01\n",
      "  1.53594595e+01  4.82887218e+00  2.25350723e+00  4.98527927e+01\n",
      "  1.10696859e+01  3.78096656e+01  1.10326030e+01 -1.90095575e-01\n",
      "  1.10326030e+01  3.72920816e+00 -1.40047322e+01  3.48050765e+01\n",
      "  6.43508350e+01  1.10326030e+01  2.44084956e+01  1.10326030e+01\n",
      "  2.02609640e+01  2.36476055e+00  1.10326030e+01  2.26720526e+00\n",
      "  2.22436673e+01  2.66568210e+00 -3.47492521e+00 -4.72847975e+01\n",
      "  3.67933060e+00  1.09959656e+02  9.91904416e+00  4.43886626e+01\n",
      "  9.00963982e+00 -2.17246247e+01  2.92874561e+01 -7.08448438e+00\n",
      "  5.38368177e+01 -2.67775578e+00  3.52360958e+01  2.15580590e+01\n",
      "  1.10326030e+01  2.07073523e+01 -1.06418175e+01  1.10326030e+01\n",
      "  1.72869227e+01 -1.39319454e+01 -1.55296118e+01  1.23604698e+01\n",
      " -5.10036138e+00  1.10326030e+01  1.10326030e+01 -1.06795758e+01\n",
      "  2.56007188e+01  2.70652122e+01  9.53182447e+00  2.12059356e+01\n",
      " -6.39424491e-02  2.30081982e+01  3.91886462e+01  4.56634542e+01\n",
      " -1.98787674e+00  6.13368558e+00 -1.50163219e+00  2.25448527e+01\n",
      "  5.42501314e+01 -1.42570719e+01  1.01133130e+01  4.62229479e+01\n",
      " -2.52566175e+01  2.34399432e+01 -8.04609610e+00  2.32999959e+01\n",
      "  1.86394478e+00  1.33286402e+01 -5.12564595e+00 -2.54606139e+01\n",
      "  1.49644438e+01  4.64399381e+01  3.60590440e+00  2.01839788e+01\n",
      "  5.44726171e+00 -1.16265606e+01  6.82504960e+01 -1.39341304e+01\n",
      " -7.01200022e+00  3.74807325e+01 -7.21529234e+00  1.10326030e+01\n",
      " -1.66704701e+01  8.94654705e+00  1.73378208e+00 -1.20641235e+01\n",
      "  2.77684824e+00  1.80734425e+01  3.52139395e+01  1.54966150e+01\n",
      "  3.73244081e+01  7.38791293e+01  3.64562056e+00  9.17576087e+00\n",
      "  1.64331834e+01  5.46207656e+00 -4.24317967e-01 -1.40463218e+01\n",
      "  2.60765421e+00  2.79423433e+01  7.63890090e+01  5.29808007e+01\n",
      "  1.45339241e+01  1.10326030e+01 -1.06795758e+01  1.51212187e+01\n",
      " -4.69203563e+00  3.91565120e+01  2.99966043e+01  1.53594595e+01\n",
      "  1.21668147e+01 -4.49319334e+00  1.44949895e+01  4.04818356e+00\n",
      "  5.82819875e+00 -1.12935223e+01  1.23164842e+01 -1.03468723e+01\n",
      "  1.07924107e+01 -3.64167977e+01  1.64510399e+01 -1.00750745e+01\n",
      "  6.64810415e+01  5.57923097e-01 -2.17701730e+01 -5.36858745e+01\n",
      "  7.58940653e+00 -1.60616614e+01 -7.75125532e+00 -4.73973014e+00\n",
      " -1.83481299e+01  7.99369791e+00 -6.40193581e+00  1.50131525e+01\n",
      " -1.06604495e+01  1.81740444e+01  1.10326030e+01  1.10326030e+01\n",
      " -3.28241496e+00 -1.01446834e+01  1.10326030e+01  3.98493899e+00\n",
      " -2.27731147e+00  1.60220576e+01  2.01465618e+01 -1.90549429e+01\n",
      " -3.93268043e+00  4.18620367e+01 -8.80418091e+01  1.81229245e+00\n",
      "  5.20198701e+00  4.18827158e+00  1.86031331e+02  1.38053149e+01\n",
      " -5.06335277e-02  1.10326030e+01  1.10326030e+01  1.10326030e+01\n",
      " -1.18463427e+01  1.68172868e+01  1.52146178e+01  3.92737398e+01\n",
      "  1.10326030e+01 -2.12216074e+00 -6.79733999e+00  1.27838757e+01\n",
      "  1.24194255e+01 -2.34174126e+01  7.65187449e+00  2.46672303e+01\n",
      " -3.81991064e+00 -2.30533253e+01 -7.63536831e+01 -4.77542280e+00\n",
      " -3.46347724e+01  5.49816156e+00 -3.99678261e+01  2.32490674e+00\n",
      "  4.46496795e+01  6.85177520e+01  2.22377429e+01  1.84964563e+01\n",
      "  6.54366457e+00 -3.77221796e+01 -1.48742304e+01  1.35902679e+00\n",
      " -8.25970105e+00 -6.36961387e+00  1.52387717e+01  1.15129488e+01\n",
      "  6.68703570e+00  1.90312988e-02  4.80855883e+00  7.45810132e+00\n",
      "  7.92665239e+00  1.06609039e+01 -2.35672080e+01  8.98636541e+00\n",
      " -1.90751081e+00 -1.57372845e+01 -4.24437615e+00  4.60569610e+00\n",
      " -7.76888940e-01  3.69802960e+00  2.65000748e+01  9.04389623e+00\n",
      "  4.74561848e+01  1.10326030e+01  8.37247247e+00  1.14857199e+01\n",
      " -8.49093586e+00 -1.50153227e+01  5.56940389e+01 -5.28496732e+00\n",
      "  7.75069869e+00  1.40513612e+01  2.07130758e+00  1.40395348e+01\n",
      " -8.29772115e+00  2.53542663e+01  9.46091090e+00  5.99143668e-01\n",
      "  1.10326030e+01  1.00930051e+02 -9.55841862e-01 -2.46061899e+01\n",
      "  4.35092221e+01  4.34977098e-01  2.69759833e+01  4.88169220e+01\n",
      " -1.05943960e+01  2.80444905e+01  3.80125134e+00 -4.81433656e+01\n",
      " -3.81091144e-01  1.45916291e+00 -3.95871430e+01  1.10326030e+01\n",
      " -1.47009026e+01  1.32344595e+01 -4.91802118e+00  1.15890139e+01\n",
      "  6.99521375e+01  3.05992655e-01  5.68225617e+01  3.08473099e+01\n",
      " -1.01218759e+01  1.44767421e+01 -5.45320566e+00 -7.12762881e+01\n",
      "  1.10326030e+01  1.10326030e+01  3.26664953e+01  1.16212082e+01\n",
      "  7.64463110e+00  1.62291941e+01  3.36584514e+01  3.44363766e+01\n",
      " -1.24065120e+01  8.17998035e+00 -8.08715541e+00  2.47194047e+00\n",
      "  1.10326030e+01  2.12188271e+00 -1.10807837e+01  1.95863536e+01\n",
      "  2.14808457e+01  6.67966862e-01 -1.02576453e+01  5.55078707e+00\n",
      " -1.74327623e+01 -5.56975380e+01  1.44615276e+01  5.34115319e+01\n",
      "  5.39516627e+00 -3.67928295e+00 -4.69838069e-01  6.78670467e+00\n",
      " -5.36904150e+01 -1.39150883e+01  3.18744200e-01  1.42310446e+01\n",
      "  2.20349479e+01  1.18448025e+01 -1.43513267e+01  2.11886963e+01\n",
      "  1.10326030e+01  3.85545618e+01 -7.77822019e+00 -4.58007106e+01\n",
      " -2.06809704e+01  7.73740713e+00 -3.08557575e+00  3.78948890e+00\n",
      "  6.07749591e+00 -6.88279625e+00  2.38923831e+01  3.11760269e+01\n",
      "  3.24106060e+01 -2.89957779e+00 -6.61124733e+00  1.33936105e+01\n",
      "  1.10219071e+01 -1.99572856e+01  1.19903559e+01  3.00687375e+00\n",
      "  1.55575871e+01 -2.42900739e+00 -3.84689018e+01  2.01192421e+01\n",
      " -2.53923588e+00  3.06590765e+01  3.03072008e+00  3.12223009e+01\n",
      "  4.18230964e+01 -6.51698926e+01 -1.18699527e+01  1.10326030e+01\n",
      "  1.15072300e+01  6.19846523e+00 -2.51687658e+01  9.29967639e+00\n",
      "  2.40295278e+01  1.80986853e+01  2.80192271e+00  7.10851525e+01\n",
      "  7.66174501e+01 -2.92475403e+01 -3.70191233e+01 -2.21375942e+00\n",
      " -1.27930777e+01 -5.08819659e+01  1.15129936e+01 -1.51676003e+01\n",
      "  9.29967639e+00  1.10326030e+01  1.10326030e+01  1.10326030e+01\n",
      " -2.12882365e+01  8.46944876e+00 -1.02236924e+01  3.29536587e+01\n",
      " -2.60281440e+01 -8.65123812e+00 -2.35944689e+01  1.96854547e+01\n",
      "  1.63970999e+01 -8.36957339e+00 -6.86433338e+00 -4.18094355e-01\n",
      "  1.28078089e+01 -2.37240790e+00  2.33409992e+00 -4.39434887e+00\n",
      "  1.10326030e+01  2.15562852e+01  9.43084285e+00 -1.33208319e+01\n",
      "  2.73700021e+01 -1.61997379e+01  8.26181415e+00  9.92842456e+00\n",
      "  8.26538229e+00  4.87912929e+01 -4.20792877e+01 -1.35218978e+01\n",
      "  1.75418994e+01  6.42156613e+00  1.15415739e+01 -1.39155810e+01\n",
      "  7.75268382e+00  1.10326030e+01  2.92734949e+01 -2.35769607e+01\n",
      " -2.07230335e+01  1.23604698e+01 -2.63639292e+01  1.08614778e+01\n",
      "  1.51963429e+01  1.07120168e+02  4.69278722e+00  4.08619086e+00\n",
      " -6.24589112e+00  6.68703570e+00  5.94467224e+01 -1.37444794e+01\n",
      "  1.30484346e+01 -2.73962934e+01  9.86915663e+00  6.10864709e+01\n",
      " -1.60828060e+01 -1.12545976e+01  4.01329027e+01  1.51850358e+01\n",
      " -1.65828267e+01  1.43926358e+01  4.53170067e+01  1.29248342e+01\n",
      "  1.99611344e+02 -6.74055897e+00  1.67697446e+01 -7.93980045e+00\n",
      " -8.31423009e+00  3.76423844e+01  6.00221855e+00  9.20801087e+01\n",
      "  2.87426683e+01  4.68680012e+00  1.24931320e+01  2.15436493e+01\n",
      "  3.03110429e+00  5.15157148e+01  8.19051006e+00 -7.83910093e+00\n",
      "  2.51677691e+01 -1.12577470e+01 -9.38722698e+00  1.10326030e+01\n",
      "  3.70406802e+01  1.10326030e+01 -1.71327617e+01  4.83216816e+01\n",
      "  9.17576087e+00 -7.05789133e+00  1.17567034e+01  1.10326030e+01\n",
      "  4.10421266e+01  1.10326030e+01  4.20292267e+01  1.01676771e+01\n",
      " -6.88971143e+00  2.04358102e+01 -1.55071822e+01 -4.65352528e+01\n",
      "  1.03197917e+00  4.31412804e+01 -4.05900454e+01  1.10326030e+01\n",
      "  4.43828818e+01  7.86334451e+01  1.48832347e+01  2.36291738e+01\n",
      "  5.43182660e+00  1.73237240e+01 -2.33638956e+01  1.10326030e+01\n",
      "  1.10326030e+01  3.09635627e+01  8.34003389e+01  1.54996586e+00\n",
      " -2.72073743e+00  7.34933712e+00 -3.38090687e+01  1.15415739e+01\n",
      " -2.51665810e+01  1.23167307e+01  3.93878142e+00  6.06062836e+00\n",
      "  2.28665571e+01  1.43170650e-01  4.71251468e+01 -4.55326116e+01\n",
      "  4.80556822e+00  1.46627846e+01  4.59604539e+01  2.55021181e+00\n",
      "  1.73860980e+01  1.00740710e+01  1.10326030e+01  1.10326030e+01\n",
      " -2.31932296e+01  5.54935489e+01  1.10326030e+01 -3.35327699e+00\n",
      "  1.60882639e+01 -1.10718476e+01  4.15852731e+00  2.41014544e+01\n",
      " -3.31575486e+01 -1.08411104e+00  3.96810416e+00 -8.39919754e+00\n",
      "  7.16979839e+00  2.07286449e-01  7.34516760e+00  4.24865168e+00\n",
      "  3.09983132e+01 -1.91097722e+01  1.10326030e+01 -9.30938697e-01\n",
      " -1.24119514e-01 -6.11262866e+00 -8.32074609e+00  1.74405172e+01\n",
      "  7.12916321e+00  1.10326030e+01 -6.22317998e-01  1.96500160e+00\n",
      " -1.09097148e+01 -8.17464358e+00 -2.75681640e+01  2.84570065e+01\n",
      "  8.71204878e+00  1.53594595e+01  2.68396443e+01  1.10326030e+01\n",
      "  1.02632602e+01  1.10326030e+01  1.10326030e+01  1.60261890e+01\n",
      "  1.10326030e+01  1.68947606e+01  4.84308855e+00 -7.88163014e+00\n",
      "  9.42180289e+00  2.56292245e+01  2.60676998e+01  1.14037021e+01\n",
      "  2.44139291e+01  2.24180345e+00  9.65453005e+00  1.18622348e+01\n",
      " -1.69851265e+01 -8.90285258e+00  2.25350723e+00  1.08951358e+01\n",
      "  9.57655535e+00 -1.71551701e+01  2.85316337e+01  8.35524070e+01\n",
      " -4.32016252e+01 -6.39424491e-02  2.90438998e+01 -5.04219804e+01\n",
      " -9.55841862e-01 -8.18711276e+00  2.80081376e+01 -8.25970105e+00\n",
      "  1.09864162e+01  3.25312328e-02  1.83527128e+00  7.62370773e+01\n",
      "  1.10326030e+01 -2.32518821e+01 -9.48792840e+00  3.54687338e+00\n",
      " -1.34657540e+01  6.78676574e+00  4.53028412e+01 -2.80181701e+01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Prediction Error\n",
    "\n",
    "Now that we have predictions, we can calculate our prediction error. We'll use mean squared error (MSE).\n",
    "\n",
    "With MSE, we subtract the predictions from the actual values, square the results, and find the mean. Because the errors are squared, MSE penalizes errors further away from the actual value more than those close to the actual value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.145705668968\n"
     ]
    }
   ],
   "source": [
    "mse = sum((predictions - y_test) ** 2) / len(predictions)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the square root of our MSE to calculate error in terms of upvotes, we get 51.5. This means that our average error is 51.5 upvotes away from the true value. We have high error in predicting upvotes as we have used a very small data set. With larger training sets, this should decrease dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.48927757959872\n"
     ]
    }
   ],
   "source": [
    "mse = sum((predictions - y_test) ** 2) / len(predictions)\n",
    "rmse = (mse)**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
